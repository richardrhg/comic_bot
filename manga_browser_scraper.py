import time
import json
import os
import requests
from datetime import datetime
from urllib.parse import urlparse, urljoin
import argparse
import csv

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
except ImportError:
    print("âŒ éœ€è¦å®‰è£selenium: pip install selenium")
    exit(1)

class MangaBrowserScraper:
    def __init__(self, headless=False, wait_time=5, auto_download=False, output_dir="downloaded_images", delay=0.5):
        """åˆå§‹åŒ–ç€è¦½å™¨çˆ¬èŸ²"""
        self.wait_time = wait_time
        self.driver = None
        self.auto_download = auto_download
        self.output_dir = output_dir
        self.delay = delay
        self.setup_driver(headless)
        
        # è¨­ç½®ä¸‹è¼‰ç›¸é—œ
        if self.auto_download:
            self.setup_downloader()
    
    def setup_downloader(self):
        """è¨­ç½®ä¸‹è¼‰å™¨"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://ganma.jp/',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8'
        })
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(self.output_dir, exist_ok=True)
        print(f"ğŸ“ è‡ªå‹•ä¸‹è¼‰æ¨¡å¼å·²å•Ÿç”¨ï¼Œåœ–ç‰‡å°‡ä¿å­˜åˆ°: {os.path.abspath(self.output_dir)}")
    
    def setup_driver(self, headless):
        """è¨­ç½®Chromeç€è¦½å™¨é©…å‹•"""
        try:
            chrome_options = Options()
            
            if headless:
                chrome_options.add_argument("--headless")
            
            # åŸºæœ¬è¨­ç½®
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            print("âœ… Chromeç€è¦½å™¨å•Ÿå‹•æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ ç€è¦½å™¨å•Ÿå‹•å¤±æ•—: {e}")
            print("è«‹ç¢ºä¿å·²å®‰è£Chromeç€è¦½å™¨å’ŒChromeDriver")
            exit(1)
    
    def scrape_manga_page(self, url):
        """çˆ¬å–æ¼«ç•«é é¢"""
        try:
            print(f"ğŸŒ æ­£åœ¨è¨ªå•: {url}")
            self.driver.get(url)
            
            # ç­‰å¾…é é¢åŠ è¼‰
            print("â³ ç­‰å¾…é é¢åŠ è¼‰...")
            time.sleep(self.wait_time)
            
            # ç­‰å¾…JavaScriptåŸ·è¡Œå®Œæˆ
            self.wait_for_page_load()
            
            # ç²å–é é¢ä¿¡æ¯
            page_info = self.extract_page_info(url)
            
            # ç²å–æ‰€æœ‰åœ–ç‰‡
            images = self.extract_all_images()
            
            # ä¿å­˜çµæœ
            self.save_results(url, page_info, images)
            
            # å¦‚æœå•Ÿç”¨è‡ªå‹•ä¸‹è¼‰ï¼Œç›´æ¥ä¸‹è¼‰åœ–ç‰‡
            if self.auto_download and images:
                self.download_images_auto(images, page_info)
            
            return {
                'page_info': page_info,
                'images': images
            }
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±æ•—: {e}")
            return None
    
    def wait_for_page_load(self):
        """ç­‰å¾…é é¢å®Œå…¨åŠ è¼‰"""
        try:
            # ç­‰å¾…bodyå…ƒç´ å‡ºç¾
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ç­‰å¾…é é¢åŠ è¼‰å®Œæˆ
            WebDriverWait(self.driver, 10).until(
                lambda driver: driver.execute_script("return document.readyState") == "complete"
            )
            
            # é¡å¤–ç­‰å¾…JavaScriptåŸ·è¡Œ
            time.sleep(2)
            
        except TimeoutException:
            print("âš ï¸ é é¢åŠ è¼‰è¶…æ™‚ï¼Œç¹¼çºŒåŸ·è¡Œ...")
    
    def extract_page_info(self, url):
        """æå–é é¢åŸºæœ¬ä¿¡æ¯"""
        try:
            page_info = {
                'url': url,
                'title': self.driver.title,
                'timestamp': datetime.now().isoformat(),
                'user_agent': self.driver.execute_script("return navigator.userAgent"),
                'viewport': self.driver.execute_script("return window.innerWidth + 'x' + window.innerHeight")
            }
            
            # å˜—è©¦ç²å–æ¼«ç•«æ¨™é¡Œ - æ”¹é€²æ¨™é¡Œæå–é‚è¼¯
            manga_title = self.extract_manga_title()
            if manga_title:
                page_info['manga_title'] = manga_title
            else:
                # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ¼«ç•«æ¨™é¡Œï¼Œä½¿ç”¨é é¢æ¨™é¡Œä½œç‚ºå‚™é¸
                page_title = self.driver.title.strip()
                if page_title and page_title != "N/A":
                    # æ¸…ç†é é¢æ¨™é¡Œï¼Œç§»é™¤ç¶²ç«™åç¨±ç­‰
                    clean_title = self.clean_page_title(page_title)
                    page_info['manga_title'] = clean_title
                else:
                    # ä½¿ç”¨åŸŸåä½œç‚ºæœ€å¾Œå‚™é¸
                    domain = urlparse(url).netloc.replace('.', '_')
                    page_info['manga_title'] = domain
            
            return page_info
            
        except Exception as e:
            print(f"âš ï¸ æå–é é¢ä¿¡æ¯å¤±æ•—: {e}")
            # å³ä½¿å‡ºéŒ¯ä¹Ÿè¦æä¾›ä¸€å€‹æœ‰æ„ç¾©çš„æ¨™é¡Œ
            domain = urlparse(url).netloc.replace('.', '_')
            return {
                'url': url, 
                'timestamp': datetime.now().isoformat(),
                'manga_title': domain
            }
    
    def extract_manga_title(self):
        """æ”¹é€²çš„æ¼«ç•«æ¨™é¡Œæå–æ–¹æ³•"""
        try:
            # å˜—è©¦å¤šç¨®é¸æ“‡å™¨ä¾†æ‰¾åˆ°æ¼«ç•«æ¨™é¡Œ
            selectors = [
                "h1", 
                ".title", 
                ".manga-title", 
                ".comic-title",
                ".story-title",
                "[class*='title']",
                "[class*='manga']",
                "[class*='comic']",
                "[class*='story']",
                "title"  # é é¢æ¨™é¡Œ
            ]
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = element.text.strip()
                        if text and len(text) > 2 and len(text) < 100:  # åˆç†çš„æ¨™é¡Œé•·åº¦
                            # éæ¿¾æ‰æ˜é¡¯ä¸æ˜¯æ¨™é¡Œçš„å…§å®¹
                            if not any(skip in text.lower() for skip in ['login', 'sign', 'menu', 'nav', 'footer']):
                                print(f"âœ… æ‰¾åˆ°æ¼«ç•«æ¨™é¡Œ: {text}")
                                return text
                except:
                    continue
            
            # å¦‚æœCSSé¸æ“‡å™¨éƒ½æ²’æ‰¾åˆ°ï¼Œå˜—è©¦å¾é é¢æ¨™é¡Œæå–
            page_title = self.driver.title.strip()
            if page_title and page_title != "N/A":
                clean_title = self.clean_page_title(page_title)
                if clean_title:
                    print(f"âœ… å¾é é¢æ¨™é¡Œæå–: {clean_title}")
                    return clean_title
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ æå–æ¼«ç•«æ¨™é¡Œæ™‚å‡ºéŒ¯: {e}")
            return None
    
    def clean_page_title(self, title):
        """æ¸…ç†é é¢æ¨™é¡Œï¼Œæå–æœ‰ç”¨çš„éƒ¨åˆ†"""
        try:
            # ç§»é™¤å¸¸è¦‹çš„ç¶²ç«™å¾Œç¶´
            suffixes = [
                ' - ganma.jp',
                ' | ganma.jp',
                ' - Ganma!',
                ' | Ganma!',
                ' - ã‚¬ãƒ³ãƒ',
                ' | ã‚¬ãƒ³ãƒ',
                ' - æ¼«ç”»',
                ' | æ¼«ç”»',
                ' - ãƒãƒ³ã‚¬',
                ' | ãƒãƒ³ã‚¬'
            ]
            
            clean_title = title
            for suffix in suffixes:
                if clean_title.endswith(suffix):
                    clean_title = clean_title[:-len(suffix)].strip()
                    break
            
            # å¦‚æœæ¨™é¡Œå¤ªé•·ï¼Œæˆªå–å‰50å€‹å­—ç¬¦
            if len(clean_title) > 50:
                clean_title = clean_title[:50].strip()
            
            return clean_title if clean_title else None
            
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ¨™é¡Œæ™‚å‡ºéŒ¯: {e}")
            return None
    
    def extract_all_images(self):
        """æå–é é¢ä¸­çš„æ‰€æœ‰åœ–ç‰‡"""
        images = []
        
        try:
            # 1. æå–imgæ¨™ç±¤åœ–ç‰‡
            img_images = self.extract_img_tags()
            images.extend(img_images)
            
            # 2. æå–canvaså…ƒç´ 
            canvas_images = self.extract_canvas_elements()
            images.extend(canvas_images)
            
            # 3. æå–èƒŒæ™¯åœ–ç‰‡
            background_images = self.extract_background_images()
            images.extend(background_images)
            
            # 4. æå–SVGå…ƒç´ 
            svg_images = self.extract_svg_elements()
            images.extend(svg_images)
            
            # 5. æå–pictureæ¨™ç±¤
            picture_images = self.extract_picture_elements()
            images.extend(picture_images)
            
            # 6. å¾JavaScriptè®Šé‡ä¸­æå–åœ–ç‰‡URL
            js_images = self.extract_js_image_urls()
            images.extend(js_images)
            
            print(f"âœ… ç¸½å…±æ‰¾åˆ° {len(images)} å€‹åœ–ç‰‡å…ƒç´ ")
            
        except Exception as e:
            print(f"âš ï¸ æå–åœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return images
    
    def extract_img_tags(self):
        """æå–imgæ¨™ç±¤åœ–ç‰‡"""
        images = []
        try:
            img_elements = self.driver.find_elements(By.TAG_NAME, "img")
            print(f"æ‰¾åˆ° {len(img_elements)} å€‹imgæ¨™ç±¤")
            
            for i, img in enumerate(img_elements, 1):
                try:
                    img_info = {
                        'type': 'img_tag',
                        'index': i,
                        'src': img.get_attribute('src'),
                        'alt': img.get_attribute('alt'),
                        'title': img.get_attribute('title'),
                        'width': img.get_attribute('width'),
                        'height': img.get_attribute('height'),
                        'class': img.get_attribute('class'),
                        'id': img.get_attribute('id'),
                        'data_src': img.get_attribute('data-src'),
                        'loading': img.get_attribute('loading')
                    }
                    
                    # æ¸…ç†ç©ºå€¼
                    img_info = {k: v for k, v in img_info.items() if v}
                    images.append(img_info)
                    
                except Exception as e:
                    print(f"âš ï¸ è™•ç†imgæ¨™ç±¤ {i} æ™‚å‡ºéŒ¯: {e}")
            
        except Exception as e:
            print(f"âš ï¸ æå–imgæ¨™ç±¤å¤±æ•—: {e}")
        
        return images
    
    def extract_canvas_elements(self):
        """æå–canvaså…ƒç´ """
        images = []
        try:
            canvas_elements = self.driver.find_elements(By.TAG_NAME, "canvas")
            print(f"æ‰¾åˆ° {len(canvas_elements)} å€‹canvaså…ƒç´ ")
            
            for i, canvas in enumerate(canvas_elements, 1):
                try:
                    canvas_info = {
                        'type': 'canvas',
                        'index': i,
                        'width': canvas.get_attribute('width'),
                        'height': canvas.get_attribute('height'),
                        'class': canvas.get_attribute('class'),
                        'id': canvas.get_attribute('id'),
                        'data_url': canvas.get_attribute('data-url')
                    }
                    
                    # å˜—è©¦ç²å–canvasçš„data URL
                    try:
                        data_url = self.driver.execute_script(
                            "return arguments[0].toDataURL('image/png');", canvas
                        )
                        if data_url and data_url.startswith('data:image'):
                            canvas_info['data_url'] = data_url[:100] + '...' if len(data_url) > 100 else data_url
                    except:
                        pass
                    
                    # æ¸…ç†ç©ºå€¼
                    canvas_info = {k: v for k, v in canvas_info.items() if v}
                    images.append(canvas_info)
                    
                except Exception as e:
                    print(f"âš ï¸ è™•ç†canvas {i} æ™‚å‡ºéŒ¯: {e}")
            
        except Exception as e:
            print(f"âš ï¸ æå–canvaså¤±æ•—: {e}")
        
        return images
    
    def extract_background_images(self):
        """æå–èƒŒæ™¯åœ–ç‰‡"""
        images = []
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½æœ‰èƒŒæ™¯åœ–ç‰‡çš„å…ƒç´ 
            elements_with_bg = self.driver.find_elements(
                By.CSS_SELECTOR, 
                "[style*='background'], [style*='background-image']"
            )
            print(f"æ‰¾åˆ° {len(elements_with_bg)} å€‹å¸¶èƒŒæ™¯æ¨£å¼çš„å…ƒç´ ")
            
            for i, element in enumerate(elements_with_bg, 1):
                try:
                    style = element.get_attribute('style')
                    if style and ('background' in style.lower() or 'background-image' in style.lower()):
                        bg_info = {
                            'type': 'background_image',
                            'index': i,
                            'style': style,
                            'tag': element.tag_name,
                            'class': element.get_attribute('class'),
                            'id': element.get_attribute('id')
                        }
                        images.append(bg_info)
                        
                except Exception as e:
                    print(f"âš ï¸ è™•ç†èƒŒæ™¯æ¨£å¼ {i} æ™‚å‡ºéŒ¯: {e}")
            
        except Exception as e:
            print(f"âš ï¸ æå–èƒŒæ™¯åœ–ç‰‡å¤±æ•—: {e}")
        
        return images
    
    def extract_svg_elements(self):
        """æå–SVGå…ƒç´ """
        images = []
        try:
            svg_elements = self.driver.find_elements(By.TAG_NAME, "svg")
            print(f"æ‰¾åˆ° {len(svg_elements)} å€‹SVGå…ƒç´ ")
            
            for i, svg in enumerate(svg_elements, 1):
                try:
                    svg_info = {
                        'type': 'svg',
                        'index': i,
                        'width': svg.get_attribute('width'),
                        'height': svg.get_attribute('height'),
                        'class': svg.get_attribute('class'),
                        'id': svg.get_attribute('id'),
                        'content': svg.get_attribute('outerHTML')[:200] + '...' if len(svg.get_attribute('outerHTML')) > 200 else svg.get_attribute('outerHTML')
                    }
                    images.append(svg_info)
                    
                except Exception as e:
                    print(f"âš ï¸ è™•ç†SVG {i} æ™‚å‡ºéŒ¯: {e}")
            
        except Exception as e:
            print(f"âš ï¸ æå–SVGå¤±æ•—: {e}")
        
        return images
    
    def extract_picture_elements(self):
        """æå–pictureæ¨™ç±¤"""
        images = []
        try:
            picture_elements = self.driver.find_elements(By.TAG_NAME, "picture")
            print(f"æ‰¾åˆ° {len(picture_elements)} å€‹pictureæ¨™ç±¤")
            
            for i, picture in enumerate(picture_elements, 1):
                try:
                    # æŸ¥æ‰¾pictureå…§çš„img
                    img = picture.find_element(By.TAG_NAME, "img")
                    if img:
                        picture_info = {
                            'type': 'picture',
                            'index': i,
                            'src': img.get_attribute('src'),
                            'srcset': img.get_attribute('srcset'),
                            'sizes': img.get_attribute('sizes'),
                            'alt': img.get_attribute('alt')
                        }
                        images.append(picture_info)
                        
                except Exception as e:
                    print(f"âš ï¸ è™•ç†picture {i} æ™‚å‡ºéŒ¯: {e}")
            
        except Exception as e:
            print(f"âš ï¸ æå–pictureå¤±æ•—: {e}")
        
        return images
    
    def extract_js_image_urls(self):
        """å¾JavaScriptè®Šé‡ä¸­æå–åœ–ç‰‡URL"""
        images = []
        try:
            # åŸ·è¡ŒJavaScriptä¾†æŸ¥æ‰¾å¯èƒ½çš„åœ–ç‰‡URL
            js_code = """
            const imageUrls = [];
            
            // æŸ¥æ‰¾å…¨å±€è®Šé‡ä¸­çš„åœ–ç‰‡URL
            for (let key in window) {
                try {
                    const value = window[key];
                    if (typeof value === 'string' && value.includes('.jpg') || value.includes('.png') || value.includes('.webp')) {
                        if (value.startsWith('http')) {
                            imageUrls.push({
                                source: 'global_variable',
                                key: key,
                                url: value
                            });
                        }
                    }
                } catch (e) {}
            }
            
            // æŸ¥æ‰¾dataå±¬æ€§ä¸­çš„åœ–ç‰‡URL
            document.querySelectorAll('[data-*]').forEach(el => {
                for (let attr of el.attributes) {
                    if (attr.name.startsWith('data-') && (attr.value.includes('.jpg') || attr.value.includes('.png') || attr.value.includes('.webp'))) {
                        if (attr.value.startsWith('http')) {
                            imageUrls.push({
                                source: 'data_attribute',
                                element: el.tagName,
                                attribute: attr.name,
                                url: attr.value
                            });
                        }
                    }
                }
            });
            
            return imageUrls;
            """
            
            js_results = self.driver.execute_script(js_code)
            if js_results:
                print(f"å¾JavaScriptä¸­æ‰¾åˆ° {len(js_results)} å€‹åœ–ç‰‡URL")
                for result in js_results:
                    result['type'] = 'js_extracted'
                    images.append(result)
            
        except Exception as e:
            print(f"âš ï¸ å¾JavaScriptæå–åœ–ç‰‡URLå¤±æ•—: {e}")
        
        return images
    
    def download_images_auto(self, images, page_info):
        """è‡ªå‹•ä¸‹è¼‰åœ–ç‰‡"""
        print(f"\n é–‹å§‹è‡ªå‹•ä¸‹è¼‰ {len(images)} å€‹åœ–ç‰‡...")
        
        success_count = 0
        failed_count = 0
        
        # ç²å–æ¼«ç•«æ¨™é¡Œç”¨æ–¼æ–‡ä»¶å
        manga_title = page_info.get('manga_title', 'unknown')
        manga_title = self.clean_filename(manga_title)
        
        # å¦‚æœæ¨™é¡Œé‚„æ˜¯unknownï¼Œä½¿ç”¨é é¢æ¨™é¡Œæˆ–åŸŸå
        if manga_title == 'unknown' or not manga_title:
            page_title = page_info.get('title', '')
            if page_title and page_title != 'N/A':
                manga_title = self.clean_filename(self.clean_page_title(page_title))
            else:
                domain = urlparse(page_info.get('url', '')).netloc.replace('.', '_')
                manga_title = domain
        
        print(f" ä½¿ç”¨æ¨™é¡Œ: {manga_title}")
        
        for i, img in enumerate(images, 1):
            try:
                # ç²å–åœ–ç‰‡URL
                img_url = img.get('src') or img.get('url')
                if not img_url or not img_url.startswith(('http://', 'https://')):
                    print(f"[{i}/{len(images)}] âš ï¸ è·³éç„¡æ•ˆURL: {img_url}")
                    continue
                
                print(f"\n[{i}/{len(images)}] æ­£åœ¨ä¸‹è¼‰...")
                print(f"   URL: {img_url[:100]}...")
                
                # ä¸‹è¼‰åœ–ç‰‡
                response = self.session.get(img_url, timeout=30)
                
                if response.status_code == 200:
                    # æª¢æŸ¥å…§å®¹é¡å‹
                    content_type = response.headers.get('content-type', '')
                    print(f"  ğŸ“‹ å…§å®¹é¡å‹: {content_type}")
                    print(f"  ğŸ“ æ–‡ä»¶å¤§å°: {len(response.content)} bytes")
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    if 'image/' in content_type:
                        ext = content_type.split('/')[-1]
                    else:
                        ext = 'jpg'  # é»˜èªæ“´å±•å
                    
                    # ç”Ÿæˆæ›´æœ‰æ„ç¾©çš„æ–‡ä»¶å
                    filename = self.generate_meaningful_filename(img, manga_title, i, ext)
                    filepath = os.path.join(self.output_dir, filename)
                    
                    # ä¿å­˜åœ–ç‰‡
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    
                    print(f"  âœ… å·²ä¿å­˜: {filename}")
                    success_count += 1
                    
                else:
                    print(f"  âŒ HTTPéŒ¯èª¤: {response.status_code}")
                    failed_count += 1
                
                # å»¶é²é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
                if i < len(images):
                    time.sleep(self.delay)
                
            except Exception as e:
                print(f"  âŒ ä¸‹è¼‰å¤±æ•—: {e}")
                failed_count += 1
        
        # é¡¯ç¤ºä¸‹è¼‰çµæœ
        print(f"\nâœ… è‡ªå‹•ä¸‹è¼‰å®Œæˆï¼")
        print(f"æˆåŠŸ: {success_count} å€‹")
        print(f"å¤±æ•—: {failed_count} å€‹")
        print(f"åœ–ç‰‡ä¿å­˜åœ¨: {os.path.abspath(self.output_dir)}")
    
    def generate_meaningful_filename(self, img, manga_title, index, ext):
        """ç”Ÿæˆæ›´æœ‰æ„ç¾©çš„æ–‡ä»¶å"""
        try:
            # æ§‹å»ºæ–‡ä»¶åçµ„ä»¶
            parts = []
            
            # 1. æ¼«ç•«æ¨™é¡Œï¼ˆä¸»è¦çµ„ä»¶ï¼‰
            if manga_title and manga_title != 'unknown':
                parts.append(manga_title)
            
            # 2. åœ–ç‰‡é¡å‹ï¼ˆæ›´å‹å¥½çš„æè¿°ï¼‰
            img_type = img.get('type', '')
            type_mapping = {
                'img_tag': 'img',
                'canvas': 'canvas',
                'background_image': 'bg',
                'svg': 'svg',
                'picture': 'pic',
                'js_extracted': 'js'
            }
            friendly_type = type_mapping.get(img_type, img_type)
            if friendly_type:
                parts.append(friendly_type)
            
            # 3. åœ–ç‰‡ç´¢å¼•ï¼ˆä¿æŒ3ä½æ•¸æ ¼å¼ï¼‰
            parts.append(f"{index:03d}")
            
            # 4. å¦‚æœæœ‰altæ–‡æœ¬ï¼Œå¯ä»¥æ·»åŠ ï¼ˆé™åˆ¶é•·åº¦ï¼‰
            alt_text = img.get('alt', '')
            if alt_text and len(alt_text) < 20:
                clean_alt = self.clean_filename(alt_text)
                if clean_alt and clean_alt != 'unknown':
                    parts.append(clean_alt)
            
            # çµ„åˆæ–‡ä»¶å
            if parts:
                filename = "_".join(parts) + "." + ext
            else:
                filename = f"image_{index:03d}.{ext}"
            
            return filename
            
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆæ–‡ä»¶åæ™‚å‡ºéŒ¯: {e}")
            return f"image_{index:03d}.{ext}"
    
    def clean_filename(self, name):
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        if not name:
            return "unknown"
        # æ›¿æ›éæ³•å­—ç¬¦
        illegal_chars = r'<>:"/\|?*'
        for char in illegal_chars:
            name = name.replace(char, '_')
        # é™åˆ¶é•·åº¦
        if len(name) > 30:
            name = name[:30]
        return name.strip('_')
    
    def save_results(self, url, page_info, images):
        """ä¿å­˜çˆ¬å–çµæœ"""
        if not images:
            print("æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡ï¼Œè·³éä¿å­˜")
            return
        
        os.makedirs('output', exist_ok=True)
        domain = urlparse(url).netloc
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜è©³ç´°çµæœ
        detailed_file = f"output/manga_detailed_{domain}_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump({
                'page_info': page_info,
                'images': images,
                'total_images': len(images)
            }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç°¡åŒ–çµæœ
        simple_file = f"output/manga_simple_{domain}_{timestamp}.txt"
        with open(simple_file, 'w', encoding='utf-8') as f:
            f.write(f"æ¼«ç•«é é¢åœ–ç‰‡æå–çµæœ\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ç¶²å€: {url}\n")
            f.write(f"æ¨™é¡Œ: {page_info.get('title', 'N/A')}\n")
            f.write(f"æ¼«ç•«æ¨™é¡Œ: {page_info.get('manga_title', 'N/A')}\n")
            f.write(f"æå–æ™‚é–“: {page_info.get('timestamp', 'N/A')}\n")
            f.write(f"ç¸½åœ–ç‰‡æ•¸é‡: {len(images)}\n\n")
            
            # æŒ‰é¡å‹çµ±è¨ˆ
            type_counts = {}
            for img in images:
                img_type = img['type']
                type_counts[img_type] = type_counts.get(img_type, 0) + 1
            
            f.write("åœ–ç‰‡é¡å‹çµ±è¨ˆ:\n")
            for img_type, count in type_counts.items():
                f.write(f"  {img_type}: {count} å€‹\n")
            f.write("\n")
            
            # åœ–ç‰‡URLåˆ—è¡¨
            f.write("åœ–ç‰‡URLåˆ—è¡¨:\n")
            f.write("-" * 30 + "\n")
            url_count = 0
            for img in images:
                if 'src' in img and img['src']:
                    url_count += 1
                    f.write(f"{url_count}. {img['src']}\n")
                elif 'url' in img and img['url']:
                    url_count += 1
                    f.write(f"{url_count}. {img['url']}\n")
            
            if url_count == 0:
                f.write("æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„åœ–ç‰‡URL\n")
        
        # ä¿å­˜CSVæ–‡ä»¶
        csv_file = f"output/manga_images_{domain}_{timestamp}.csv"
        self.save_images_to_csv(csv_file, images, page_info)
        
        print(f"âœ… è©³ç´°çµæœå·²ä¿å­˜åˆ°: {detailed_file}")
        print(f"âœ… ç°¡åŒ–çµæœå·²ä¿å­˜åˆ°: {simple_file}")
        print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜åˆ°: {csv_file}")
    
    def save_images_to_csv(self, csv_file, images, page_info):
        """å°‡åœ–ç‰‡ä¿¡æ¯ä¿å­˜åˆ°CSVæ–‡ä»¶"""
        try:
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # å¯«å…¥æ¨™é¡Œè¡Œ
                writer.writerow([
                    'index', 'type', 'url', 'alt', 'title', 'width', 'height', 
                    'class', 'id', 'manga_title', 'page_title', 'extract_time'
                ])
                
                # å¯«å…¥åœ–ç‰‡æ•¸æ“š
                for i, img in enumerate(images, 1):
                    row = [
                        i,  # index
                        img.get('type', ''),  # type
                        img.get('src') or img.get('url', ''),  # url
                        img.get('alt', ''),  # alt
                        img.get('title', ''),  # title
                        img.get('width', ''),  # width
                        img.get('height', ''),  # height
                        img.get('class', ''),  # class
                        img.get('id', ''),  # id
                        page_info.get('manga_title', ''),  # manga_title
                        page_info.get('title', ''),  # page_title
                        page_info.get('timestamp', '')  # extract_time
                    ]
                    writer.writerow(row)
                    
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜CSVæ–‡ä»¶å¤±æ•—: {e}")
    
    def print_summary(self, page_info, images):
        """æ‰“å°çˆ¬å–æ‘˜è¦"""
        print(f"\nâœ… æ¼«ç•«é é¢çˆ¬å–å®Œæˆï¼")
        print(f"æ¨™é¡Œ: {page_info.get('title', 'N/A')}")
        print(f"æ¼«ç•«æ¨™é¡Œ: {page_info.get('manga_title', 'N/A')}")
        print(f"ç¸½å…±æ‰¾åˆ° {len(images)} å€‹åœ–ç‰‡å…ƒç´ ")
        
        # æŒ‰é¡å‹çµ±è¨ˆ
        type_counts = {}
        for img in images:
            img_type = img['type']
            type_counts[img_type] = type_counts.get(img_type, 0) + 1
        
        print("\nåœ–ç‰‡é¡å‹çµ±è¨ˆ:")
        for img_type, count in type_counts.items():
            print(f"  {img_type}: {count} å€‹")
        
        # é¡¯ç¤ºå‰å¹¾å€‹åœ–ç‰‡
        print(f"\nå‰5å€‹åœ–ç‰‡é è¦½:")
        for i, img in enumerate(images[:5], 1):
            if 'src' in img and img['src']:
                print(f"  {i}. [{img['type']}] {img['src']}")
            elif 'url' in img and img['url']:
                print(f"  {i}. [{img['type']}] {img['url']}")
            else:
                print(f"  {i}. [{img['type']}] ç„¡URL")
    
    def close(self):
        """é—œé–‰ç€è¦½å™¨"""
        if self.driver:
            self.driver.quit()
            print(" ç€è¦½å™¨å·²é—œé–‰")

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='æ¼«ç•«ç€è¦½å™¨çˆ¬èŸ² - æ•´åˆç‰ˆæœ¬ï¼Œæ”¯æŒè‡ªå‹•ä¸‹è¼‰')
    parser.add_argument('-u', '--url', required=True, help='æ¼«ç•«é é¢URL')
    parser.add_argument('--headless', action='store_true', help='ç„¡é ­æ¨¡å¼ï¼ˆä¸é¡¯ç¤ºç€è¦½å™¨çª—å£ï¼‰')
    parser.add_argument('--wait', type=int, default=5, help='ç­‰å¾…é é¢åŠ è¼‰çš„æ™‚é–“ï¼ˆç§’ï¼‰')
    parser.add_argument('--auto-download', action='store_true', help='çˆ¬å–å®Œæˆå¾Œè‡ªå‹•ä¸‹è¼‰åœ–ç‰‡')
    parser.add_argument('--output-dir', default='downloaded_images', help='åœ–ç‰‡ä¸‹è¼‰ç›®éŒ„')
    parser.add_argument('--delay', type=float, default=0.5, help='ä¸‹è¼‰å»¶é²æ™‚é–“ï¼ˆç§’ï¼‰')
    
    args = parser.parse_args()
    
    scraper = None
    try:
        scraper = MangaBrowserScraper(
            headless=args.headless, 
            wait_time=args.wait,
            auto_download=args.auto_download,
            output_dir=args.output_dir,
            delay=args.delay
        )
        
        result = scraper.scrape_manga_page(args.url)
        
        if result:
            scraper.print_summary(result['page_info'], result['images'])
            
            if args.auto_download:
                print(f"\n åœ–ç‰‡å·²è‡ªå‹•ä¸‹è¼‰åˆ°: {os.path.abspath(args.output_dir)}")
            else:
                print(f"\nğŸ“ åœ–ç‰‡ä¿¡æ¯å·²ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼Œå¯ä½¿ç”¨ download_form_csv.py é€²è¡Œä¸‹è¼‰")
        else:
            print("âŒ çˆ¬å–å¤±æ•—")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        if scraper:
            scraper.close()

if __name__ == "__main__":
    main()
